
__device__ inline bool cinn_nvgpu_bitwise_and_bool(bool a, bool b) { return a & b; }
__device__ inline bool cinn_nvgpu_bitwise_or_bool(bool a, bool b) { return a | b; }
__device__ inline bool cinn_nvgpu_bitwise_xor_bool(bool a, bool b) { return a ^ b; }
__device__ inline bool cinn_nvgpu_bitwise_not_bool(bool a) { return !a; }

__device__ inline uint8_t cinn_nvgpu_bitwise_and_uint8(uint8_t a, uint8_t b) { return a & b; }
__device__ inline uint8_t cinn_nvgpu_bitwise_or_uint8(uint8_t a, uint8_t b) { return a | b; }
__device__ inline uint8_t cinn_nvgpu_bitwise_xor_uint8(uint8_t a, uint8_t b) { return a ^ b; }
__device__ inline uint8_t cinn_nvgpu_bitwise_not_uint8(uint8_t a) { return ~a; }
__device__ inline uint8_t cinn_nvgpu_logical_right_shift_uint8(uint8_t a, uint8_t b) { return ((uint8_t)a >> b); }

__device__ inline int8_t cinn_nvgpu_bitwise_and_int8(int8_t a, int8_t b) { return a & b; }
__device__ inline int8_t cinn_nvgpu_bitwise_or_int8(int8_t a, int8_t b) { return a | b; }
__device__ inline int8_t cinn_nvgpu_bitwise_xor_int8(int8_t a, int8_t b) { return a ^ b; }
__device__ inline int8_t cinn_nvgpu_bitwise_not_int8(int8_t a) { return ~a; }
__device__ inline int8_t cinn_nvgpu_logical_right_shift_int8(int8_t a, int8_t b) { return ((uint8_t)a >> b); }

__device__ inline int16_t cinn_nvgpu_bitwise_and_int16(int16_t a, int16_t b) { return a & b; }
__device__ inline int16_t cinn_nvgpu_bitwise_or_int16(int16_t a, int16_t b) { return a | b; }
__device__ inline int16_t cinn_nvgpu_bitwise_xor_int16(int16_t a, int16_t b) { return a ^ b; }
__device__ inline int16_t cinn_nvgpu_bitwise_not_int16(int16_t a) { return ~a; }
__device__ inline int16_t cinn_nvgpu_logical_right_shift_int16(int16_t a, int16_t b) { return ((uint16_t)a >> b); }

__device__ inline float cinn_nvgpu_sin_fp32(float x) { return sin(x); }
__device__ inline float cinn_nvgpu_cos_fp32(float x) { return cos(x); }
__device__ inline float cinn_nvgpu_tan_fp32(float x) { return tan(x); }
__device__ inline float cinn_nvgpu_sinh_fp32(float x) { return sinh(x); }
__device__ inline float cinn_nvgpu_cosh_fp32(float x) { return cosh(x); }
__device__ inline float cinn_nvgpu_tanh_fp32(float x) { return tanh(x); }

__device__ inline float cinn_nvgpu_asin_fp32(float x) { return asin(x); }
__device__ inline float cinn_nvgpu_acos_fp32(float x) { return acos(x); }
__device__ inline float cinn_nvgpu_atan_fp32(float x) { return atan(x); }
__device__ inline float cinn_nvgpu_asinh_fp32(float x) { return asinh(x); }
__device__ inline float cinn_nvgpu_acosh_fp32(float x) { return acosh(x); }
__device__ inline float cinn_nvgpu_atanh_fp32(float x) { return atanh(x); }

__device__ inline float cinn_nvgpu_ceil_fp32(float x) { return ceil(x); }
__device__ inline float cinn_nvgpu_round_fp32(float x) { return round(x); }
__device__ inline float cinn_nvgpu_trunc_fp32(float x) { return trunc(x); }
__device__ inline float cinn_nvgpu_abs_fp32(float x) { return abs(x); }
__device__ inline float cinn_nvgpu_floor_fp32(float x) { return floor(x); }
__device__ inline float cinn_nvgpu_log_fp32(float x) { return log(x); }
__device__ inline float cinn_nvgpu_log2_fp32(float x) { return log2(x); }
__device__ inline float cinn_nvgpu_log10_fp32(float x) { return log10(x); }
__device__ inline float cinn_nvgpu_exp_fp32(float x) { return exp(x); }
__device__ inline float cinn_nvgpu_erf_fp32(float x) { return erf(x); }
__device__ inline float cinn_nvgpu_sigmoid_fp32(float x) { return 1.0f / (1.0f + exp(-x)); }
__device__ inline float cinn_nvgpu_sqrt_fp32(float x) { return sqrt(x); }
__device__ inline float cinn_nvgpu_rsqrt_fp32(float x) { return rsqrt(x); }
__device__ inline float cinn_nvgpu_cbrt_fp32(float x) { return cbrt(x); }

__device__ inline bool cinn_nvgpu_isfinite_fp32(float x) { return isfinite(x); }
__device__ inline bool cinn_nvgpu_isinf_fp32(float x) { return isinf(x); }
__device__ inline bool cinn_nvgpu_isnan_fp32(float x) { return isnan(x); }

__device__ inline float cinn_nvgpu_pow_fp32(float a, float b) { return powf(a, b); }
__device__ inline float cinn_nvgpu_mod_fp32(float a, float b) { float res = fmodf(a, b); if ((res != 0.0f) && ((res < 0.0f) != (b < 0.0f))) res += b; return res; }
__device__ inline float cinn_nvgpu_rcp_fp32(float x) { float res; asm("rcp.approx.ftz.f32 %0, %1;" : "=f"(res) : "f"(x)); return res; }

__device__ inline double cinn_nvgpu_sin_fp64(double x) { return sin(x); }
__device__ inline double cinn_nvgpu_cos_fp64(double x) { return cos(x); }
__device__ inline double cinn_nvgpu_tan_fp64(double x) { return tan(x); }
__device__ inline double cinn_nvgpu_sinh_fp64(double x) { return sinh(x); }
__device__ inline double cinn_nvgpu_cosh_fp64(double x) { return cosh(x); }
__device__ inline double cinn_nvgpu_tanh_fp64(double x) { return tanh(x); }

__device__ inline double cinn_nvgpu_asin_fp64(double x) { return asin(x); }
__device__ inline double cinn_nvgpu_acos_fp64(double x) { return acos(x); }
__device__ inline double cinn_nvgpu_atan_fp64(double x) { return atan(x); }
__device__ inline double cinn_nvgpu_asinh_fp64(double x) { return asinh(x); }
__device__ inline double cinn_nvgpu_acosh_fp64(double x) { return acosh(x); }
__device__ inline double cinn_nvgpu_atanh_fp64(double x) { return atanh(x); }

__device__ inline double cinn_nvgpu_ceil_fp64(double x) { return ceil(x); }
__device__ inline double cinn_nvgpu_round_fp64(double x) { return round(x); }
__device__ inline double cinn_nvgpu_trunc_fp64(double x) { return trunc(x); }
__device__ inline double cinn_nvgpu_abs_fp64(double x) { return abs(x); }
__device__ inline double cinn_nvgpu_floor_fp64(double x) { return floor(x); }
__device__ inline double cinn_nvgpu_log_fp64(double x) { return log(x); }
__device__ inline double cinn_nvgpu_log2_fp64(double x) { return log2(x); }
__device__ inline double cinn_nvgpu_log10_fp64(double x) { return log10(x); }
__device__ inline double cinn_nvgpu_exp_fp64(double x) { return exp(x); }
__device__ inline double cinn_nvgpu_erf_fp64(double x) { return erf(x); }
__device__ inline double cinn_nvgpu_sigmoid_fp64(double x) { return 1.0 / (1.0 + exp(-x)); }
__device__ inline double cinn_nvgpu_sqrt_fp64(double x) { return sqrt(x); }
__device__ inline double cinn_nvgpu_rsqrt_fp64(double x) { return rsqrt(x); }
__device__ inline double cinn_nvgpu_cbrt_fp64(double x) { return cbrt(x); }

__device__ inline bool cinn_nvgpu_isfinite_fp64(double x) { return isfinite(x); }
__device__ inline bool cinn_nvgpu_isinf_fp64(double x) { return isinf(x); }
__device__ inline bool cinn_nvgpu_isnan_fp64(double x) { return isnan(x); }

__device__ inline double cinn_nvgpu_pow_fp64(double a, double b) { return pow(a, b); }
__device__ inline double cinn_nvgpu_mod_fp64(double a, double b) { double res = fmod(a, b); if ((res != 0.0) && ((res < 0.0) != (b < 0.0))) res += b; return res; }
__device__ inline double cinn_nvgpu_rcp_fp64(double x) { double res; asm("rcp.approx.ftz.f64 %0, %1;" : "=d"(res) : "d"(x)); return res; }

struct welford_fp32 { float mean; float m2; float weight; __device__ welford_fp32() {}; __device__ explicit welford_fp32(float value) : mean(value), m2(0), weight(1) {} __device__ welford_fp32(float mean, float m2, float weight) : mean(mean), m2(m2), weight(weight) {} __device__ explicit operator float() const { return m2 / weight; } }; __device__ inline welford_fp32 operator+(const welford_fp32& a, const welford_fp32& b) { float delta = b.mean - a.mean; float weight = a.weight + b.weight; float mean, m2; if (b.weight == 1) { mean = a.mean + delta * cinn_nvgpu_rcp_fp32(weight); m2 = a.m2 + delta * (b.mean - mean); } else { float w2_over_w = a.weight == b.weight ? (float)0.5 : b.weight * cinn_nvgpu_rcp_fp32(weight); mean = a.mean + delta * w2_over_w; m2 = a.m2 + b.m2 + delta * delta * a.weight * w2_over_w; } return {mean, m2, weight}; } __device__ inline welford_fp32 __shfl_down_sync(unsigned mask, const welford_fp32& var, unsigned delta, int width = 32) { float mean = __shfl_down_sync(mask, var.mean, delta, width); float m2 = __shfl_down_sync(mask, var.m2, delta, width); float weight = __shfl_down_sync(mask, var.weight, delta, width); return {mean, m2, weight}; } __device__ inline welford_fp32 __shfl_xor_sync(unsigned mask, const welford_fp32& var, int laneMask, int width = 32) { float mean = __shfl_xor_sync(mask, var.mean, laneMask, width); float m2 = __shfl_xor_sync(mask, var.m2, laneMask, width); float weight = __shfl_xor_sync(mask, var.weight, laneMask, width); return {mean, m2, weight}; }
struct welford_fp64 { double mean; double m2; double weight; __device__ welford_fp64() {}; __device__ explicit welford_fp64(double value) : mean(value), m2(0), weight(1) {} __device__ welford_fp64(double mean, double m2, double weight) : mean(mean), m2(m2), weight(weight) {} __device__ explicit operator double() const { return m2 / weight; } }; __device__ inline welford_fp64 operator+(const welford_fp64& a, const welford_fp64& b) { double delta = b.mean - a.mean; double weight = a.weight + b.weight; double mean, m2; if (b.weight == 1) { mean = a.mean + delta * cinn_nvgpu_rcp_fp64(weight); m2 = a.m2 + delta * (b.mean - mean); } else { double w2_over_w = a.weight == b.weight ? (double)0.5 : b.weight * cinn_nvgpu_rcp_fp64(weight); mean = a.mean + delta * w2_over_w; m2 = a.m2 + b.m2 + delta * delta * a.weight * w2_over_w; } return {mean, m2, weight}; } __device__ inline welford_fp64 __shfl_down_sync(unsigned mask, const welford_fp64& var, unsigned delta, int width = 32) { double mean = __shfl_down_sync(mask, var.mean, delta, width); double m2 = __shfl_down_sync(mask, var.m2, delta, width); double weight = __shfl_down_sync(mask, var.weight, delta, width); return {mean, m2, weight}; } __device__ inline welford_fp64 __shfl_xor_sync(unsigned mask, const welford_fp64& var, int laneMask, int width = 32) { double mean = __shfl_xor_sync(mask, var.mean, laneMask, width); double m2 = __shfl_xor_sync(mask, var.m2, laneMask, width); double weight = __shfl_xor_sync(mask, var.weight, laneMask, width); return {mean, m2, weight}; }

__device__ inline int cinn_nvgpu_pow_int32(int a, int b) { if (a == 0 && b < 0) { return -1; } float res = pow(__int2float_rd(a), __int2float_rd(b)); return __float2int_rn(res); }

__device__ inline int cinn_nvgpu_left_shift_int32(int a, int b) { return a << b; }
__device__ inline int cinn_nvgpu_right_shift_int32(int a, int b) { return a >> b; }
__device__ inline int cinn_nvgpu_bitwise_and_int32(int a, int b) { return a & b; }
__device__ inline int cinn_nvgpu_bitwise_or_int32(int a, int b) { return a | b; }
__device__ inline int cinn_nvgpu_bitwise_xor_int32(int a, int b) { return a ^ b; }
__device__ inline int cinn_nvgpu_bitwise_not_int32(int a) { return ~a; }
__device__ inline int cinn_nvgpu_clz_int32(int a) { return __clz(a); }
__device__ inline int cinn_nvgpu_popc_int32(int a) { return __popc(a); }
__device__ inline int cinn_nvgpu_logical_right_shift_int32(int a, int b) { return ((unsigned int)a >> b); }
__device__ inline int cinn_nvgpu_trunc_int32(int a) { return a; }

__device__ inline int cinn_nvgpu_max_int32(int a, int b) { return max(a, b); }
__device__ inline int cinn_nvgpu_min_int32(int a, int b) { return min(a, b); }

__device__ inline int cinn_nvgpu_mod_int32(int a, int b) { int res = a % b; if ((res != 0) && ((b ^ res) < 0)) res += b; return res; }

__device__ inline long long int cinn_nvgpu_bitwise_and_int64(long long int a, long long int b) { return a & b; }
__device__ inline long long int cinn_nvgpu_bitwise_or_int64(long long int a, long long int b) { return a | b; }
__device__ inline long long int cinn_nvgpu_bitwise_xor_int64(long long int a, long long int b) { return a ^ b; }
__device__ inline long long int cinn_nvgpu_bitwise_not_int64(long long int a) { return ~a; }
__device__ inline long long int cinn_nvgpu_clz_int64(long long int a) { return __clzll(a); }
__device__ inline long long int cinn_nvgpu_popc_int64(long long int a) { return __popcll(a); }
__device__ inline long long int cinn_nvgpu_logical_right_shift_int64(long long int a, long long int b) { return ((unsigned long long int)a >> b); }
__device__ inline long long int cinn_nvgpu_trunc_int64(long long int a) { return a; }
__device__ inline long long int cinn_nvgpu_mod_int64(long long int a, long long int b) { long long int res = a % b; if ((res != 0) && ((b ^ res) < 0)) res += b; return res; }
__device__ inline long long int cinn_nvgpu_pow_int64(long long int a, long long int b) { double res = pow(__ll2double_rd(a), __ll2double_rd(b)); return __double2ll_rn(res); }

__device__ inline half cinn_nvgpu_ceil_fp16(half x) { return half(hceil(x)); }
__device__ inline half cinn_nvgpu_floor_fp16(half x) { return half(hfloor(x)); }
__device__ inline half cinn_nvgpu_round_fp16(half x) { return half(cinn_nvgpu_round_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_trunc_fp16(half x) { return half(htrunc(x)); }

__device__ inline half cinn_nvgpu_sin_fp16(half x) { return half(hsin(x)); }
__device__ inline half cinn_nvgpu_cos_fp16(half x) { return half(hcos(x)); }

__device__ inline half cinn_nvgpu_exp_fp16(half x) { return half(hexp(x)); }
__device__ inline half cinn_nvgpu_log_fp16(half x) { return half(hlog(x)); }
__device__ inline half cinn_nvgpu_log2_fp16(half x) { return half(hlog2(x)); }
__device__ inline half cinn_nvgpu_log10_fp16(half x) { return half(hlog10(x)); }

__device__ inline half cinn_nvgpu_sqrt_fp16(half x) { return half(hsqrt(x)); }
__device__ inline half cinn_nvgpu_rsqrt_fp16(half x) { return half(hrsqrt(x)); }

__device__ inline half cinn_nvgpu_cbrt_fp16(half x) { return half(cinn_nvgpu_cbrt_fp32(static_cast<float>(x))); }

__device__ inline half cinn_nvgpu_abs_fp16(half x) { return __habs(x); }

__device__ inline bool cinn_nvgpu_isnan_fp16(half x) { return __hisnan(x); }
__device__ inline bool cinn_nvgpu_isinf_fp16(half x) { return __hisinf(x); }
__device__ inline bool cinn_nvgpu_isfinite_fp16(half x) { return !__hisnan(x) && !__hisinf(x); }

__device__ inline half cinn_nvgpu_erf_fp16(half x) { return half(cinn_nvgpu_erf_fp32(static_cast<float>(x))); }

__device__ inline half cinn_nvgpu_tan_fp16(half x) { return half(cinn_nvgpu_tan_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_sinh_fp16(half x) { return half(cinn_nvgpu_sinh_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_cosh_fp16(half x) { return half(cinn_nvgpu_cosh_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_tanh_fp16(half x) { return half(cinn_nvgpu_tanh_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_asin_fp16(half x) { return half(cinn_nvgpu_asin_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_acos_fp16(half x) { return half(cinn_nvgpu_acos_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_atan_fp16(half x) { return half(cinn_nvgpu_atan_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_asinh_fp16(half x) { return half(cinn_nvgpu_asinh_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_acosh_fp16(half x) { return half(cinn_nvgpu_acosh_fp32(static_cast<float>(x))); }
__device__ inline half cinn_nvgpu_atanh_fp16(half x) { return half(cinn_nvgpu_atanh_fp32(static_cast<float>(x))); }

__device__ inline half cinn_nvgpu_sigmoid_fp16(half x) { return half(cinn_nvgpu_sigmoid_fp32(static_cast<float>(x))); }

__device__ inline half cinn_nvgpu_mod_fp16(half a, half b) { return half(cinn_nvgpu_mod_fp32(static_cast<float>(a), static_cast<float>(b))); }
__device__ inline half cinn_nvgpu_pow_fp16(half a, half b) { return half(cinn_nvgpu_pow_fp32(static_cast<float>(a), static_cast<float>(b))); }

__device__ inline int cinn_sum_int32(const int left, const int right) { return left + right; }
__device__ inline int cinn_prod_int32(const int left, const int right) { return left * right; }
__device__ inline int cinn_max_int32(const int left, const int right) { return max(left, right); }
__device__ inline int cinn_min_int32(const int left, const int right) { return min(left, right); }

__device__ inline long long int cinn_sum_int64(const long long int left, const long long int right) { return left + right; }
__device__ inline long long int cinn_prod_int64(const long long int left, const long long int right) { return left * right; }
__device__ inline long long int cinn_max_int64(const long long int left, const long long int right) { return max(left, right); }
__device__ inline long long int cinn_min_int64(const long long int left, const long long int right) { return min(left, right); }

__device__ inline float cinn_sum_fp32(const float left, const float right) { return left + right; }
__device__ inline float cinn_prod_fp32(const float left, const float right) { return left * right; }
__device__ inline float cinn_max_fp32(const float left, const float right) { return max(left, right); }
__device__ inline float cinn_min_fp32(const float left, const float right) { return min(left, right); }
__device__ inline welford_fp32 cinn_sum_welford_fp32(welford_fp32 left, welford_fp32 right) { return left + right; }

__device__ inline half cinn_sum_fp16(const half left, const half right) { return left + right; }
__device__ inline half cinn_prod_fp16(const half left, const half right) { return left * right; }
__device__ inline half cinn_max_fp16(const half left, const half right) { return __hmax(left, right); }
__device__ inline half cinn_min_fp16(const half left, const half right) { return __hmin(left, right); }

__device__ inline double cinn_sum_fp64(const double left, const double right) { return left + right; }
__device__ inline double cinn_prod_fp64(const double left, const double right) { return left * right; }
__device__ inline double cinn_max_fp64(const double left, const double right) { return max(left, right); }
__device__ inline double cinn_min_fp64(const double left, const double right) { return min(left, right); }
__device__ inline welford_fp64 cinn_sum_welford_fp64(welford_fp64 left, welford_fp64 right) { return left + right; }

__device__ inline bool cinn_all(const bool left, const bool right) { return left && right; }
__device__ inline bool cinn_any(const bool left, const bool right) { return left || right; }

__device__ inline int cinn_warp_shuffle_sum_int32_internal(const int value) { int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (int)(0); } tmp_val = cinn_sum_int32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_int32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline int cinn_warp_shuffle_prod_int32_internal(const int value) { int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (int)(1); } tmp_val = cinn_prod_int32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_prod_int32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline int cinn_warp_shuffle_max_int32_internal(const int value) { int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (int)(-2147483648); } tmp_val = cinn_max_int32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_max_int32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline int cinn_warp_shuffle_min_int32_internal(const int value) { int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (int)(2147483647); } tmp_val = cinn_min_int32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_min_int32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline long long int cinn_warp_shuffle_sum_int64_internal(const long long int value) { long long int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { long long int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (long long int)(0); } tmp_val = cinn_sum_int64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_int64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline long long int cinn_warp_shuffle_prod_int64_internal(const long long int value) { long long int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { long long int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (long long int)(1); } tmp_val = cinn_prod_int64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_prod_int64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline long long int cinn_warp_shuffle_max_int64_internal(const long long int value) { long long int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { long long int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (long long int)(-9223372036854775808); } tmp_val = cinn_max_int64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_max_int64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline long long int cinn_warp_shuffle_min_int64_internal(const long long int value) { long long int tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { long long int shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (long long int)(9223372036854775807); } tmp_val = cinn_min_int64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_min_int64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline float cinn_warp_shuffle_sum_fp32_internal(const float value) { float tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { float shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (float)(0.0f); } tmp_val = cinn_sum_fp32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_fp32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline float cinn_warp_shuffle_prod_fp32_internal(const float value) { float tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { float shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (float)(1.0f); } tmp_val = cinn_prod_fp32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_prod_fp32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline float cinn_warp_shuffle_max_fp32_internal(const float value) { float tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { float shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (float)(-3.40282e+38f); } tmp_val = cinn_max_fp32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_max_fp32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline float cinn_warp_shuffle_min_fp32_internal(const float value) { float tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { float shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (float)(3.40282e+38f); } tmp_val = cinn_min_fp32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_min_fp32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline welford_fp32 cinn_warp_shuffle_sum_welford_fp32_internal(const welford_fp32 value) { welford_fp32 tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { welford_fp32 shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (welford_fp32)(welford_fp32(0.0f, 0.0f, 0.0f)); } tmp_val = cinn_sum_welford_fp32(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_welford_fp32(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline double cinn_warp_shuffle_sum_fp64_internal(const double value) { double tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { double shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (double)(0.0); } tmp_val = cinn_sum_fp64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_fp64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline double cinn_warp_shuffle_prod_fp64_internal(const double value) { double tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { double shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (double)(1.0); } tmp_val = cinn_prod_fp64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_prod_fp64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline double cinn_warp_shuffle_max_fp64_internal(const double value) { double tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { double shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (double)(-1.79769e+308); } tmp_val = cinn_max_fp64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_max_fp64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline double cinn_warp_shuffle_min_fp64_internal(const double value) { double tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { double shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (double)(1.79769e+308); } tmp_val = cinn_min_fp64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_min_fp64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline welford_fp64 cinn_warp_shuffle_sum_welford_fp64_internal(const welford_fp64 value) { welford_fp64 tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { welford_fp64 shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (welford_fp64)(welford_fp64(0.0, 0.0, 0.0)); } tmp_val = cinn_sum_welford_fp64(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_welford_fp64(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline bool cinn_warp_shuffle_all_internal(const bool value) { bool tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { bool shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (bool)(true); } tmp_val = cinn_all(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_all(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline bool cinn_warp_shuffle_any_internal(const bool value) { bool tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { bool shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (bool)(false); } tmp_val = cinn_any(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_any(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }

__device__ inline half cinn_warp_shuffle_sum_fp16_internal(const half value) { half tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { half shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (half)(half(0.0)); } tmp_val = cinn_sum_fp16(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_sum_fp16(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline half cinn_warp_shuffle_prod_fp16_internal(const half value) { half tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { half shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (half)(half(1.0)); } tmp_val = cinn_prod_fp16(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_prod_fp16(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline half cinn_warp_shuffle_max_fp16_internal(const half value) { half tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { half shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (half)(__ushort_as_half(0xfbff)); } tmp_val = cinn_max_fp16(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_max_fp16(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }
__device__ inline half cinn_warp_shuffle_min_fp16_internal(const half value) { half tmp_val = value; unsigned int mask = __activemask(); unsigned int lane = __popc(mask); if (lane < 32) { for (int offset = 16; offset > 0; offset >>= 1) { half shfl_res = __shfl_down_sync(mask, tmp_val, offset); if ((threadIdx.x & 0x1f) + offset >= lane) { shfl_res = (half)(__ushort_as_half(0x7bff)); } tmp_val = cinn_min_fp16(tmp_val, shfl_res); } } else { for (int offset = 16; offset > 0; offset >>= 1) { tmp_val = cinn_min_fp16(tmp_val, __shfl_xor_sync(mask, tmp_val, offset)); } } return tmp_val; }

__device__ inline int cinn_block_reduce_sum_int32(const int value, int* shm, bool return_warp = false) { int tmp_val = cinn_warp_shuffle_sum_int32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_int32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline int cinn_block_reduce_prod_int32(const int value, int* shm, bool return_warp = false) { int tmp_val = cinn_warp_shuffle_prod_int32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_prod_int32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline int cinn_block_reduce_max_int32(const int value, int* shm, bool return_warp = false) { int tmp_val = cinn_warp_shuffle_max_int32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_max_int32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline int cinn_block_reduce_min_int32(const int value, int* shm, bool return_warp = false) { int tmp_val = cinn_warp_shuffle_min_int32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_min_int32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline long long int cinn_block_reduce_sum_int64(const long long int value, long long int* shm, bool return_warp = false) { long long int tmp_val = cinn_warp_shuffle_sum_int64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_int64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline long long int cinn_block_reduce_prod_int64(const long long int value, long long int* shm, bool return_warp = false) { long long int tmp_val = cinn_warp_shuffle_prod_int64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_prod_int64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline long long int cinn_block_reduce_max_int64(const long long int value, long long int* shm, bool return_warp = false) { long long int tmp_val = cinn_warp_shuffle_max_int64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_max_int64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline long long int cinn_block_reduce_min_int64(const long long int value, long long int* shm, bool return_warp = false) { long long int tmp_val = cinn_warp_shuffle_min_int64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_min_int64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline float cinn_block_reduce_sum_fp32(const float value, float* shm, bool return_warp = false) { float tmp_val = cinn_warp_shuffle_sum_fp32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_fp32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline float cinn_block_reduce_prod_fp32(const float value, float* shm, bool return_warp = false) { float tmp_val = cinn_warp_shuffle_prod_fp32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_prod_fp32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline float cinn_block_reduce_max_fp32(const float value, float* shm, bool return_warp = false) { float tmp_val = cinn_warp_shuffle_max_fp32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_max_fp32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline float cinn_block_reduce_min_fp32(const float value, float* shm, bool return_warp = false) { float tmp_val = cinn_warp_shuffle_min_fp32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_min_fp32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline welford_fp32 cinn_block_reduce_sum_welford_fp32(const welford_fp32 value, welford_fp32* shm, bool return_warp = false) { welford_fp32 tmp_val = cinn_warp_shuffle_sum_welford_fp32_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_welford_fp32_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline double cinn_block_reduce_sum_fp64(const double value, double* shm, bool return_warp = false) { double tmp_val = cinn_warp_shuffle_sum_fp64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_fp64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline double cinn_block_reduce_prod_fp64(const double value, double* shm, bool return_warp = false) { double tmp_val = cinn_warp_shuffle_prod_fp64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_prod_fp64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline double cinn_block_reduce_max_fp64(const double value, double* shm, bool return_warp = false) { double tmp_val = cinn_warp_shuffle_max_fp64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_max_fp64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline double cinn_block_reduce_min_fp64(const double value, double* shm, bool return_warp = false) { double tmp_val = cinn_warp_shuffle_min_fp64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_min_fp64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline welford_fp64 cinn_block_reduce_sum_welford_fp64(const welford_fp64 value, welford_fp64* shm, bool return_warp = false) { welford_fp64 tmp_val = cinn_warp_shuffle_sum_welford_fp64_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_welford_fp64_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline bool cinn_block_reduce_all(const bool value, bool* shm, bool return_warp = false) { bool tmp_val = cinn_warp_shuffle_all_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_all_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline bool cinn_block_reduce_any(const bool value, bool* shm, bool return_warp = false) { bool tmp_val = cinn_warp_shuffle_any_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_any_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }

__device__ inline half cinn_block_reduce_sum_fp16(const half value, half* shm, bool return_warp = false) { half tmp_val = cinn_warp_shuffle_sum_fp16_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_sum_fp16_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline half cinn_block_reduce_prod_fp16(const half value, half* shm, bool return_warp = false) { half tmp_val = cinn_warp_shuffle_prod_fp16_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_prod_fp16_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline half cinn_block_reduce_max_fp16(const half value, half* shm, bool return_warp = false) { half tmp_val = cinn_warp_shuffle_max_fp16_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_max_fp16_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }
__device__ inline half cinn_block_reduce_min_fp16(const half value, half* shm, bool return_warp = false) { half tmp_val = cinn_warp_shuffle_min_fp16_internal(value); if (return_warp || blockDim.x <= 32) { return tmp_val; } __syncthreads(); if (threadIdx.x % 32 == 0) { shm[threadIdx.x / 32] = tmp_val; } __syncthreads(); if (threadIdx.x < (blockDim.x + 31) / 32) { tmp_val = cinn_warp_shuffle_min_fp16_internal(shm[threadIdx.x]); if (threadIdx.x == 0) { shm[0] = tmp_val; } } __syncthreads(); return shm[0]; }

__device__ inline int cinn_discrete_reduce_sum_int32(const int value, int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_int32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline int cinn_discrete_reduce_prod_int32(const int value, int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_prod_int32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline int cinn_discrete_reduce_max_int32(const int value, int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_max_int32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline int cinn_discrete_reduce_min_int32(const int value, int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_min_int32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline long long int cinn_discrete_reduce_sum_int64(const long long int value, long long int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_int64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline long long int cinn_discrete_reduce_prod_int64(const long long int value, long long int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_prod_int64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline long long int cinn_discrete_reduce_max_int64(const long long int value, long long int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_max_int64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline long long int cinn_discrete_reduce_min_int64(const long long int value, long long int* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_min_int64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline float cinn_discrete_reduce_sum_fp32(const float value, float* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_fp32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline float cinn_discrete_reduce_prod_fp32(const float value, float* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_prod_fp32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline float cinn_discrete_reduce_max_fp32(const float value, float* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_max_fp32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline float cinn_discrete_reduce_min_fp32(const float value, float* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_min_fp32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline welford_fp32 cinn_discrete_reduce_sum_welford_fp32(const welford_fp32 value, welford_fp32* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_welford_fp32(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline double cinn_discrete_reduce_sum_fp64(const double value, double* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_fp64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline double cinn_discrete_reduce_prod_fp64(const double value, double* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_prod_fp64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline double cinn_discrete_reduce_max_fp64(const double value, double* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_max_fp64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline double cinn_discrete_reduce_min_fp64(const double value, double* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_min_fp64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline welford_fp64 cinn_discrete_reduce_sum_welford_fp64(const welford_fp64 value, welford_fp64* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_welford_fp64(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline bool cinn_discrete_reduce_all(const bool value, bool* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_all(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline bool cinn_discrete_reduce_any(const bool value, bool* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_any(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }

__device__ inline half cinn_discrete_reduce_sum_fp16(const half value, half* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_sum_fp16(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline half cinn_discrete_reduce_prod_fp16(const half value, half* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_prod_fp16(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline half cinn_discrete_reduce_max_fp16(const half value, half* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_max_fp16(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline half cinn_discrete_reduce_min_fp16(const half value, half* shm) { int tid = threadIdx.y * blockDim.x + threadIdx.x; __syncthreads(); shm[tid] = value; __syncthreads(); for (int offset = blockDim.y / 2; offset > 0; offset >>= 1) { if (threadIdx.y < offset) { shm[tid] = cinn_min_fp16(shm[tid], shm[tid + offset * blockDim.x]); } __syncthreads(); } return shm[threadIdx.x]; }
__device__ inline int cinn_grid_reduce_sum_int32(const int* mem, int spatial_size, int spatial_index) { int tmp_val = (int)(0); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_int32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline int cinn_grid_reduce_prod_int32(const int* mem, int spatial_size, int spatial_index) { int tmp_val = (int)(1); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_prod_int32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline int cinn_grid_reduce_max_int32(const int* mem, int spatial_size, int spatial_index) { int tmp_val = (int)(-2147483648); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_max_int32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline int cinn_grid_reduce_min_int32(const int* mem, int spatial_size, int spatial_index) { int tmp_val = (int)(2147483647); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_min_int32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline long long int cinn_grid_reduce_sum_int64(const long long int* mem, int spatial_size, int spatial_index) { long long int tmp_val = (long long int)(0); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_int64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline long long int cinn_grid_reduce_prod_int64(const long long int* mem, int spatial_size, int spatial_index) { long long int tmp_val = (long long int)(1); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_prod_int64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline long long int cinn_grid_reduce_max_int64(const long long int* mem, int spatial_size, int spatial_index) { long long int tmp_val = (long long int)(-9223372036854775808); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_max_int64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline long long int cinn_grid_reduce_min_int64(const long long int* mem, int spatial_size, int spatial_index) { long long int tmp_val = (long long int)(9223372036854775807); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_min_int64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline float cinn_grid_reduce_sum_fp32(const float* mem, int spatial_size, int spatial_index) { float tmp_val = (float)(0.0f); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_fp32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline float cinn_grid_reduce_prod_fp32(const float* mem, int spatial_size, int spatial_index) { float tmp_val = (float)(1.0f); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_prod_fp32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline float cinn_grid_reduce_max_fp32(const float* mem, int spatial_size, int spatial_index) { float tmp_val = (float)(-3.40282e+38f); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_max_fp32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline float cinn_grid_reduce_min_fp32(const float* mem, int spatial_size, int spatial_index) { float tmp_val = (float)(3.40282e+38f); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_min_fp32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline welford_fp32 cinn_grid_reduce_sum_welford_fp32(const welford_fp32* mem, int spatial_size, int spatial_index) { welford_fp32 tmp_val = (welford_fp32)(welford_fp32(0.0f, 0.0f, 0.0f)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_welford_fp32(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline double cinn_grid_reduce_sum_fp64(const double* mem, int spatial_size, int spatial_index) { double tmp_val = (double)(0.0); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_fp64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline double cinn_grid_reduce_prod_fp64(const double* mem, int spatial_size, int spatial_index) { double tmp_val = (double)(1.0); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_prod_fp64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline double cinn_grid_reduce_max_fp64(const double* mem, int spatial_size, int spatial_index) { double tmp_val = (double)(-1.79769e+308); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_max_fp64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline double cinn_grid_reduce_min_fp64(const double* mem, int spatial_size, int spatial_index) { double tmp_val = (double)(1.79769e+308); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_min_fp64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline welford_fp64 cinn_grid_reduce_sum_welford_fp64(const welford_fp64* mem, int spatial_size, int spatial_index) { welford_fp64 tmp_val = (welford_fp64)(welford_fp64(0.0, 0.0, 0.0)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_welford_fp64(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline bool cinn_grid_reduce_all(const bool* mem, int spatial_size, int spatial_index) { bool tmp_val = (bool)(true); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_all(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline bool cinn_grid_reduce_any(const bool* mem, int spatial_size, int spatial_index) { bool tmp_val = (bool)(false); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_any(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }

__device__ inline half cinn_grid_reduce_sum_fp16(const half* mem, int spatial_size, int spatial_index) { half tmp_val = (half)(half(0.0)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_sum_fp16(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline half cinn_grid_reduce_prod_fp16(const half* mem, int spatial_size, int spatial_index) { half tmp_val = (half)(half(1.0)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_prod_fp16(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline half cinn_grid_reduce_max_fp16(const half* mem, int spatial_size, int spatial_index) { half tmp_val = (half)(__ushort_as_half(0xfbff)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_max_fp16(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }
__device__ inline half cinn_grid_reduce_min_fp16(const half* mem, int spatial_size, int spatial_index) { half tmp_val = (half)(__ushort_as_half(0x7bff)); for (int y = 0; y < gridDim.y; y++) { tmp_val = cinn_min_fp16(tmp_val, mem[y * spatial_size + spatial_index]); } return tmp_val; }

__device__ inline bool cinn_grid_reduce_update_semaphore(int *semaphores) { __shared__ bool done; __threadfence(); __syncthreads(); if (threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0) { int old = atomicAdd(&semaphores[blockIdx.x], 1); done = (old == (gridDim.y - 1)); } __syncthreads(); return done; }

#define CINN_ENTAIL_LOOP_CONDITION(__loop_var, __cond, __stride) } for (decltype(__stride) __loop_var = 0; __cond; __loop_var += __stride) {
